Here are the instructions for the intern. Please, let me know if you have any questions! Thanks for your help!
 
 
Running the Current Configuration (Train from scratch or Imagenet weights):
1. go into the shared workspace on Livermore Compute Lab (Livermore Computing - Lawrence Livermore National ...Lawrence Livermore National Laboratory (.gov)https://computing.llnl.gov) with terminal command:
cd p/lustre1/nifoi/robin-amh
2. run amh-code.py with the amh-code.cmd file with SLURM (https://hpc.llnl.gov/banks-jobs/running-jobs/slurm-commands) terminal command:
sbatch amh-code.cmd
3. check number and status of job you’re running with SLURM terminal commands:
sacct -X
vi <job-number-shown>.out 
 
The <job-number-shown>.out will show each accuracy update (if the best-score increased in the given epoch) and corresponding confusion matrix.
 
JIT (prod format: torch.jit.tracePyTorchhttps://pytorch.org › docs › stable › generated › torch....) Best, First, Best, and Last states will save to a directory called states (https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) in a folder labeled with the current date string that is generated by the amh-code.py.
Each state will be named with  <month-day-score-type>.pt and have the following dictionary keys/values:
‘model_state_dict’: model.sate_dict(), ‘optimizer_state_dict’: optimizer.state_dict()
First and last states will also contain a key for the int number ‘epochs_passed’ to keep track when more epochs are required (such as when you need to train the model for more than the maximum possible that it will run on LLNL HPC).
Note: Using this code, training will stop automatically after 20 hours as it is meant to run on LC which has a max training time of 24 hours (so if it were to go over you will lose all of your results)
 
Updating the Config File (and/or re-starting training from saved states):
The code will run the config file: amh-code-config.txt.
The only thing you probably need to change/check when running this with OMF is that
the path to the training and testing datasets are correct (which are the first lines on the configuration file), and that
the CSV for OMF has the same column and label type (0,1) for image paths and labels.
It should be very easy to add more/less training transforms, another model, and/or modify any hyperparameters from this, as well, if needed (otherwise let me know). (However, I think OMF is also binary classification so loss/optimization functions are probably good. The softmax and cross entropy loss lines are in the code but commented out.)
Of course we need a separate copy of the code for OMF. Please, let us know if any other changes are required or if you have any questions!
